<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sreeharsha Paruchuri</title>

    <meta name="author" content="Sreeharsha Paruchuri">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favi.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="styles.css">
    <script src="script.js"></script>
    
  </head>

  <body>
    <!-- Navigation Bar -->
    <div class="nav-container">
      <nav class="navbar">
        <a href="#about" class="nav-link">About</a>
        <a href="#education" class="nav-link">Education</a>
        <a href="#experience" class="nav-link">Experience</a>
        <a href="#projects" class="nav-link">Projects</a>
        <a href="#teaching" class="nav-link">Teaching</a>
        <a href="quotes.html" class="nav-link">Quotes</a>
        <button class="theme-toggle" onclick="toggleTheme()">ðŸŒ™</button>
      </nav>
    </div>

    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <div id="about" class="about-section">
            <div class="about-content">
              <div class="about-text">
                <p class="name" style="text-align: center;">
                  Sreeharsha Paruchuri
                </p>
                <p>
		I'm a Master's student in Robotic Systems Development at <a href="https://www.ri.cmu.edu/" target="_blank">Carnegie Mellon University</a>, where I focus on computer vision, robotics, and AI. I have experience in multimodal learning, reinforcement learning, and 3D vision systems.
		
		Previously, I worked as a Pre-Doctoral Research Fellow at <a href="https://www.tcs.com/what-we-do/research" target="_blank">TCS Research</a> and as a Research Assistant at the <a href="https://robotics.iiit.ac.in/" target="_blank">Robotics Research Center, IIIT-H</a>. My research interests include embodied AI, computer vision, and autonomous systems.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sparuchu@cs.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sreeharshaparuchuri">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/sreeharshaparuchuri">Github</a> &nbsp;/&nbsp;
                  <a href="documents/sreeharsha_resume_cmu.pdf">Resume</a>
                </p>
              </div>
              <div class="profile-picture-container">
                <a href="images/profile_pictures/sreeharsha_profile_picture.JPG">
                  <img alt="profile photo" src="images/profile_pictures/sreeharsha_profile_picture.JPG">
                </a>
              </div>
            </div>
          </div>

          <!-- Education Section -->
          <table id="education" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
                          <tr class="section-row">
                <td class="section-image-cell">
                  <img src='images/logos/cmu.jpeg' alt="Carnegie Mellon University Logo" class="education-logo">
                </td>
              <td class="section-content-cell">
                <strong class="section-title">Carnegie Mellon University, School of Computer Science</strong><br>
                <em>Master of Science in Robotic Systems Development (MRSD)</em><br>
                <strong>CGPA: 4.11/4.0</strong> | Expected May 2026<br><br>
                <strong>Teaching:</strong> Introduction to Deep Learning (11-785)<br>
                <strong>Coursework:</strong> <span id="cmu-courses-short">Learning for 3D Vision, Generative AI, Deep RL</span><span id="cmu-courses-full" style="display:none;">Learning for 3D Vision, Generative Artificial Intelligence, Deep Reinforcement Learning and Control, Advanced Computer Vision, Manipulation, Estimation and Control</span>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleCourses('cmu'); return false;" id="cmu-toggle">Show more</a>
                </div>
              </td>
            </tr>

            <tr class="section-row">
                              <td class="section-image-cell">
                  <img src='images/logos/iiith-v3.png' alt="IIIT Hyderabad Logo" class="education-logo">
                </td>
              <td class="section-content-cell">
                <strong class="section-title">International Institute of Information Technology, Hyderabad</strong><br>
                <em>Bachelor of Technology in Electronics and Communication Engineering (Honours)</em><br>
                <strong>Major GCPA: 9.02/10</strong> | July 2022<br><br>
                <strong>Awards:</strong> Deans Merit List, Undergraduate Research Award<br>
                <strong>Coursework:</strong> <span id="iiith-courses-short">Statistics in AI, Topics in Applied Optimization, Mobile Robotics</span><span id="iiith-courses-full" style="display:none;">Statistical Methods in Artificial Intelligence, Topics in Applied Optimization, Mobile Robotics, Data Structures and Algorithms, Digital Image Processing, Game Theory, Compilers, Operating Systems, Linear Algebra</span>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleCourses('iiith'); return false;" id="iiith-toggle">Show more</a>
                </div>
              </td>
            </tr>
          </tbody></table>

                    <!-- Experience Section -->
          <table id="experience" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Industry Experience</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/mach9-logo.jpeg' alt="Mach9 Logo" class="logo-image" onclick="toggleExperience('mach9-current')">
              </td>
              <td class="section-content-cell">
                                  <div class="experience-header">
                    <strong class="section-title">Mach9</strong>
                  <span class="experience-time">May 2025 - August 2025</span>
                </div>
                <em>Perception Software Engineering Intern</em>
                <div class="experience-subtitle">Computer Vision and Generative AI</div>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('mach9-current'); return false;" id="mach9-current-toggle">View Details</a>
                </div>
                                  <div id="mach9-current-details" class="experience-details" style="display:none;">
                    <strong>Key Contributions:</strong><br>
                    â€¢ Developed and deployed Pavement Symbol Extraction functionality to the Digital Surveyor software via <strong>CUDA-accelarated coordinate frame transformations</strong> and segmentation masks.<br>
                    â€¢ Fine-tuned a <strong>Vision-Language Model (VLM)</strong> to implement a secondary-inference pipeline to classify extracted open-set painted symbols according to user specifications.<br>
                    â€¢ Conducted 70+ controlled ablation experiments with <strong>A/B testing on Hungarian Assigner</strong> Costs, Loss weights, Model Queries and encoder-decoder expressivity to boost the performance of the production model by 4%.<br>
                    â€¢ Utilised methods from Object-Detection literature to qualitatively capture a <strong>DETR-based</strong> polyline detection model's <strong>uncertainty</strong> to expedite downstream <strong>Quality Assurance</strong> and Quality Control processes, saving company and customer resources.<br>
                    <!-- <strong>Multimodal Symbol Extraction:</strong> Designed self-correcting vector-field and DBSCAN clustering algorithm to preserve instance consistency from multi-view panoptic masks. Accelerated per-point transformations by 50Ã— via custom CUDA kernel<br>
                    â€¢ <strong>Vision-Language Model Inference:</strong> Designed orientation robust RAG pipeline for symbol classification using Gemini text embeddings and GPT-o3, achieving 85% F1 score on 20k samples<br>
                    â€¢ <strong>Uncertainty Estimation:</strong> Developed methods to quantify uncertainty in DETR-style vectorized 3D polyline predictions via self-calibration and Bayesian dropout -->
                  </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/tcs-logo.jpeg' alt="TCS Research Logo" class="logo-image" onclick="toggleExperience('tcs')">
              </td>
              <td class="section-content-cell">
                                  <div class="experience-header">
                    <strong class="section-title">Tata Consultancy Services Research</strong>
                  <span class="experience-time">July 2022 - July 2024</span>
                </div>
                <em>Pre-Doctoral Research Fellow</em>
                <div class="experience-subtitle">Reinforcement Learning and Multimodal Learning</div>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('tcs'); return false;" id="tcs-toggle">View Details</a>
                </div>
                <div id="tcs-details" class="experience-details" style="display:none;">
                  <strong>Key Contributions:</strong><br>
                  â€¢ <strong>Audio-Visual Navigation:</strong> Led development of embodied AI agent with multimodal sensing, training online RL policy with novel class-agnostic reward, reducing path length by 21%<br>
                  â€¢ <strong>Offline RL for Indoor Robot Navigation:</strong> Built simulation pipeline to collect large-scale trajectory datasets, training a Causal Decision Transformer with early multimodal fusion; integrated environment randomization, behavior cloning baselines, and replay buffer curation to improve policy robustness and sample efficiency
                  â€¢ <strong>CLIP-Enhanced Scene Graphs:</strong> Designed contrastive-learning framework to compute visual-language embeddings, leveraging GNNs to model object-region relationships<br>
                  â€¢ <strong>Open Vocabulary Manipulation (NeurIPS 23):</strong> Developed active SLAM exploration algorithm conditioned on probabilistic semantic map, improving task success by 60%
                  â€¢ Volunteered for the Project Synergy initiative by TCS wherein volunteers taught written and spoken English to students in a Bangla-medium government school.
                </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/rrc_logo.png' alt="RRC IIIT-H Logo" class="logo-image" onclick="toggleExperience('rrc')">
              </td>
              <td class="section-content-cell">
                                  <div class="experience-header">
                    <strong class="section-title">Robotics Research Center (RRC, IIIT-H)</strong>
                  <span class="experience-time">January 2020 - June 2022</span>
                </div>
                <em>Research Assistant</em>
                <div class="experience-subtitle">Computer Vision and Robotics</div>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('rrc'); return false;" id="rrc-toggle">View Details</a>
                </div>
                <div id="rrc-details" class="experience-details" style="display:none;">
                  <strong>Key Contributions:</strong><br>
                  â€¢ <strong>Autonomous Sanitization Robot:</strong> Designed and implemented end-to-end robotic system during COVID-19 to autonomously sanitize indoor spaces, integrating computer vision, Visual-SLAM, and coverage-based navigation<br>
                  â€¢ <strong>Sim-to-Real Deployment:</strong> Built Gazebo simulation environments for iterative testing, then transferred stack to hardware platform with onboard sensors and sanitization actuators; finished <em>runner-up among 140 teams</em><br>
                  â€¢ <strong>LiDAR SLAM:</strong> Evaluated LiDAR odometry and mapping approaches such as LOAM using CARLA simulation and outdoor driving data, analyzing localization accuracy and map consistency<br>
                  â€¢ <strong>Depth Estimation:</strong> Implemented stereo and monocular depth estimation methods on driving datasets including KITTI and NuScenes, developing a ROS package for multi-view bundle adjustment<br>                  
                </div>
              </td>
            </tr>
          </tbody></table>

          <!-- Additional Experience Subsection -->
          <div class="additional-experience-section">
            <h3 style="text-align: center; font-size: 18px; font-weight: 400; color: var(--text-color); margin: 20px 0 15px 0;">I have also worked at</h3>
            <div class="additional-logos-container">
              <div class="additional-item">
                <img src="images/logos/bosch_logo.png" alt="Bosch Research" class="additional-logo" onclick="toggleAdditional('bosch')" id="bosch-logo">
                <div id="bosch-details" class="additional-details">
                  <div class="experience-header">
                    <strong class="section-title">Bosch Research and Technology Center</strong>
                    <span class="experience-time">May 2021 â€“ Aug 2021</span>
                  </div>
                  <em>Software Development Engineering Intern - Computer Vision</em><br><br>
                  <strong>Key Contributions:</strong><br>
                  â€¢ Fused Laser, Camera, and Odometry data using Kalman filtering to boost online Multi-Object Tracking performance by 11% IoU on outdoor autonomous driving datasets<br>
                  â€¢ Augmented difficult-to-obtain real-world LiDAR datasets using synthetic data from generative models and physics engines, improving 3D object detection networks for outdoor scenarios
                </div>
              </div>
              
              <div class="additional-item">
                <img src="images/logos/precog_logo-800.webp" alt="PReCoG Lab" class="additional-logo" onclick="toggleAdditional('precog')" id="precog-logo">
                <div id="precog-details" class="additional-details">
                  <div class="experience-header">
                    <strong class="section-title">Precognition Lab, IIIT-H</strong>
                    <span class="experience-time">2020-2021</span>
                  </div>
                  <em>Research Assistant - Information Retrieval and Computational Social Science</em><br><br>
                  <strong>Key Contributions:</strong><br>
                  â€¢ Applied statistical machine learning with Music Information Retrieval to analyze lyrical regularities as early indicators of mental illness; Published results at INTERSPEECH 2021<br>
                  â€¢ Scraped Reddit data to link music-sharing trends with mental health during COVID-19 using BERT embeddings and DBSCAN clustering; Published in medical journal
                </div>
              </div>
            </div>
          </div>

          <!-- Projects/Research Section -->
          <table id="projects" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Projects & Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr style="background-color: var(--highlight-bg);">
      <td style="padding:16px;width:45%;vertical-align:middle">
        <div class="one">
          <img src="images/projects/ml_project.png" alt="AR Knee Surgery Project" class="project-image">
        </div>
      </td>
              <td style="padding:8px;width:55%;vertical-align:middle">
          <a href="#">
            <span class="papertitle">Augmented-Reality and Robot Assisted Knee Surgery</span>
          </a>
          <br>
          <em>MRSD Capstone Project</em>, 2024 - 2025<br>
          <em>Technologies: 3D Computer Vision, Robotics, Extended Reality, Systems Engineering</em>
          <br>
          <a href="https://mrsdprojects.ri.cmu.edu/2025teamd/">website</a>
          /
          <a href="https://github.com/KNEEpoleon">code</a>
          <p></p>
          <p>
          Gathered and analyzed requirements from user studies, market competition, and sponsors to inform system development. Processed 3D and RGB information from the Apple Vision Pro to detect bone models in the environment via ICP registration.
          </p>
        </td>
    </tr>

    <tr>
      <td style="padding:16px;width:45%;vertical-align:middle">
        <img src='images/projects/cv_project.png' alt="3D Foundation Models Project" class="project-image">
      </td>
      <td style="padding:8px;width:55%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">3D Foundation-Models for Monocular Video Reconstruction</span>
        </a>
        <br>
        <em>CMU Project</em>, 2024
        <br>
        <a href="#">report</a>
        /
        <a href="#">code</a>
        <p></p>
        <p>
        Implemented semantic-geometric feature fusion using cross-attention between foundation model embeddings (DINOv2, Depth Anything) in a hierarchical state representation to recover camera extrinsics. Devised an adaptive keyframe selection strategy for confidence-aware pointmap refinement using a DUST3R-style architecture.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:16px;width:45%;vertical-align:middle">
        <img src='images/projects/web_app.png' class="project-image">
      </td>
      <td style="padding:8px;width:55%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Neural-Assisted Depth Disparity Estimation</span>
        </a>
        <br>
        <em>Hackathon Project</em>, 2024
        <br>
        <a href="#">project demo</a>
        /
        <a href="#">code</a>
        <p></p>
        <p>
        Developed a coarse-to-fine network architecture for the OAK-D Pro that improved real-world disparity estimation quality while respecting strict onboard compute and FPS constraints. Ranked in the Top 25 teams internationally.
        </p>
      </td>
    </tr>



    <!-- <tr onmouseout="asr_stop()" onmouseover="asr_start()" bgcolor="#ffffd0">
      <td style="padding:16px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='asr_image'>
            <img src='images/asr_after.jpg' width=100%>
          </div>
          <img src='images/asr_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function asr_start() {
            document.getElementById('asr_image').style.opacity = "1";
          }

          function asr_stop() {
            document.getElementById('asr_image').style.opacity = "0";
          }
          asr_stop()
        </script>
      </td>
      <td style="padding:8px;width:65%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Automatic Speech Recognition using LSTM RNNs</span>
        </a>
        <br>
        <strong>Ishita Gupta</strong>
        <br>
        <em>CMU Course Project</em>, 2024
        <br>
        <a href="#">project page</a>
        /
        <a href="#">code</a>
        <p></p>
        <p>
        Created a phoneme recognition model with dynamic decoding (greedy and beam search), achieving a validation Levenshtein distance of 5 and validation loss of 0.31 for competitive phoneme sequence accuracy.
        </p>
      </td>
    </tr> -->

    <!-- <tr onmouseout="face_stop()" onmouseover="face_start()">
      <td style="padding:16px;width:35%;vertical-align:middle">
        <div class="one">
          <div class="two" id='face_image'>
            <img src='images/face_after.jpg' width=100%>
          </div>
          <img src='images/face_before.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function face_start() {
            document.getElementById('face_image').style.opacity = "1";
          }

          function face_stop() {
            document.getElementById('face_image').style.opacity = "0";
          }
          face_stop()
        </script>
      </td>
      <td style="padding:8px;width:65%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Face Classification and Verification Using CNNs</span>
        </a>
        <br>
        <strong>Ishita Gupta</strong>
        <br>
        <em>CMU Course Project</em>, 2024
        <br>
        <a href="#">project page</a>
        /
        <a href="#">code</a>
        <p></p>
        <p>
        Implemented and trained CNN models (ResNet34, SEResNet etc.) for face classification and verification using techniques like Transforms and CutMix achieving an 11% Equal Error Rate and 90.6% Validation accuracy.
        </p>
      </td>
    </tr> -->

    <tr style="background-color: var(--highlight-bg);">
      <td style="padding:16px;width:35%;vertical-align:middle">
        <img src='images/projects/data_viz.png' class="project-image">
      </td>
      <td style="padding:8px;width:65%;vertical-align:middle">
        <a href="#">
          <span class="papertitle">Music, Mental Health, and Representation Learning</span>
        </a>
        <br>
        <em>IIIT-H Research Project</em>, 2021-2022<br>
        <em>Published Research</em>
        <br>
        <a href="#">publication</a>
        /
        <a href="#">code</a>
        <p></p>
        <p>
        Applied BERT-based sentiment analysis and k-means clustering to uncover nuanced links between language and acoustic music features in data scraped from mental health related subreddits during COVID-19. This research contributed to understanding the relationship between music and mental health through computational methods.
        </p>
      </td>
    </tr>

          </tbody></table>

          <!-- Teaching Section -->
          <table id="teaching" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Teaching Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/cmu.jpeg' alt="Carnegie Mellon University Logo" class="teaching-logo">
              </td>
              <td class="section-content-cell">
                <strong class="section-title">Carnegie Mellon University</strong><br>
                <em>Teaching Assistant - Introduction to Deep Learning (11-785)</em><br>
                Current (as of 2025)<br>
                <a href="https://deeplearning.cs.cmu.edu/" target="_blank">Course Website</a><br><br>
                Comprehensive graduate-level course covering neural networks, CNNs, RNNs, transformers, and modern deep learning architectures. Focuses on both theoretical foundations and practical implementations using PyTorch.
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('teaching'); return false;" id="teaching-toggle">View Details</a>
                </div>
                <div id="teaching-details" class="experience-details" style="display:none;">
                  <strong>Responsibilities:</strong><br>
                  â€¢ Lead recitation sections and office hours for 200+ students<br>
                  â€¢ Grade assignments and provide detailed feedback on deep learning projects<br>
                  â€¢ Assist in course development and curriculum refinement<br>
                  â€¢ Support students with PyTorch implementations and model debugging
                </div>
              </td>
            </tr>
          </tbody></table>

          <!-- Skills Section -->
          <table id="skills" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Technical Skills</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px;">
                  <div>
                    <strong>Languages:</strong> Python, C++, MATLAB, CUDA, Java, Go, Swift<br>
                    <strong>ML/AI:</strong> PyTorch, TensorFlow, Scikit-learn, PyTorch3D
                  </div>
                  <div>
                    <strong>Tools:</strong> ROS2, Unity 3D, OpenCV, XCode, Django<br>
                    <strong>Focus:</strong> Computer Vision, Robotics, Deep Learning, 3D Vision
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  cloned from <a href="https://github.com/jonbarron/jonbarron_website">here!</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html> 